\documentclass[5p,sans,12pt,fleqn]{article}
\usepackage{times}
\usepackage{caption}
\DeclareCaptionFont{cheny}{\heiti\fontsize{10}{12}\selectfont}
\captionsetup{font = cheny}
\usepackage{mathptmx}
\usepackage{amsthm}
\hoffset=-2cm \setlength{\topmargin}{-1.0in}
\setlength{\textheight}{9.5in} \setlength{\textwidth}{6.5in}
\linespread{1.6}
\usepackage{CJK}
\usepackage{cite}
\usepackage{lastpage}
\usepackage{colortbl}
\usepackage{amssymb}
\usepackage{dsfont}
\definecolor{MSBlue}{rgb}{.204,.353,.541}
\definecolor{MSLightBlue}{rgb}{.31,.506,.741}
\definecolor{YinBlue}{rgb}{ .184,  .459,  .71}
\definecolor{tabcolor}{rgb}{ .357,  .608,  .835}
\usepackage[CJKbookmarks, colorlinks, bookmarksnumbered=true,pdfstartview=FitH,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage[english]{babel}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{ctex}
\usepackage{lipsum} % delete this as necessary
\usepackage{fancyhdr}
\usepackage{pdfpages}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{lettrine}
\usepackage{algorithm}
\usepackage{threeparttable}
\usepackage{supertabular}
\usepackage{booktabs}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{algpseudocode}
%\floatname{algorithm}{算法}
%\renewcommand{\algorithmicrequire}{\textbf{输入:}}
%\renewcommand{\algorithmicensure}{\textbf{输出:}}
\usepackage{listings}
\usepackage{framed}
\usepackage{color}
\definecolor{shadecolor}{rgb}{ .929,  .929,  .929}
\usepackage{xcolor}
\usepackage{titlesec}
\lstset{language=R}
\usepackage{natbib}
\citestyle{nature}
\setcitestyle{square}
% Set formats for each heading level
\titleformat*{\section}{\centering\Large\bfseries\sffamily\color{MSBlue}}
\titleformat*{\subsection}{\large\bfseries\sffamily\color{MSLightBlue}}
\titleformat*{\subsubsection}{\itshape\color{YinBlue}}
\definecolor{CPPLight}  {HTML} {686868}
\definecolor{CPPSteel}  {HTML} {888888}
\definecolor{CPPDark}   {HTML} {262626}
\definecolor{CPPBlue}   {HTML} {4172A3}
\definecolor{CPPGreen}  {HTML} {487818}
\definecolor{CPPBrown}  {HTML} {A07040}
\definecolor{CPPRed}    {HTML} {AD4D3A}
\definecolor{CPPViolet} {HTML} {7040A0}
\definecolor{CPPGray}  {HTML} {B8B8B8}
\lstset{
    columns=fixed,
    numbers=none,                                        % 在左侧显示行号
    frame=none,                                          % 不显示背景边框
    backgroundcolor=\color[RGB]{245,245,244},            % 设定背景颜色
    keywordstyle=\color[RGB]{40,40,255},                 % 设定关键字颜色
    numberstyle=\footnotesize\color{darkgray},           % 设定行号格式
    commentstyle=\it\color[RGB]{0,96,96},                % 设置代码注释的格式
    stringstyle=\rmfamily\slshape\color[RGB]{128,0,0},   % 设置字符串格式
    showstringspaces=false,                              % 不显示字符串中的空格
    language=R,                                          % 设置语言
    breakatwhitespace=true,
    keepspaces=true,
    stepnumber=2,
    breaklines=true,
    morekeywords={alignas,continute,friend,register,true,alignof,decltype,goto,
    reinterpret_cast,try,asm,defult,if,return,typedef,auto,delete,inline,short,
    typeid,bool,do,int,signed,typename,break,double,long,sizeof,union,case,
    dynamic_cast,mutable,static,unsigned,catch,else,namespace,static_assert,using,
    char,enum,new,static_cast,virtual,char16_t,char32_t,explict,noexcept,struct,
    void,export,nullptr,switch,volatile,class,extern,operator,template,wchar_t,
    const,false,private,this,while,constexpr,float,protected,thread_local,
    const_cast,for,public,throw,std},
    emph={map,set,multimap,multiset,unordered_map,unordered_set,
    unordered_multiset,unordered_multimap,vector,string,list,deque,
    array,stack,forwared_list,iostream,memory,shared_ptr,unique_ptr,
    random,bitset,ostream,istream,cout,cin,endl,move,default_random_engine,
    uniform_int_distribution,iterator,algorithm,functional,bing,numeric,},
    emphstyle=\color{CPPViolet},
    }
\newcounter{exhibits}
\DeclareRobustCommand{\invisiblesection}[2]{%
\refstepcounter{exhibits}
\refstepcounter{section}%
\sectionmark{#1}
\addcontentsline{toc}{section}{\protect\numberline{}#1#2}
\rhead{\textbf{#2}}
\lhead{\textbf{#1}}
%\rfoot{\textbf{2}}
%\lfoot{\textbf{2}}
\pagestyle{fancy} % for the exhibits.
\fancyfoot[LE,RO]{\bfseries \thepage~of~\pageref{LastPage}}
\fancyfoot[CO,CE]{}
\fancyfoot[LO,RE]{\bfseries My Document}
}



\newcommand{\exName}[3]{#1~#2~#3}
%\def\thesection{~\Roman{section}}

\pagestyle{fancy}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\fancypagestyle{plain}{%
\fancyhead{} % get rid of headers
\renewcommand{\headrulewidth}{0pt} % and the line
}


%\renewcommand\figurename{图}
%\renewcommand\tablename{表}
\usepackage{times}
\pagestyle{fancy}
\rhead{}
\renewcommand{\headrulewidth}{0pt}
\usepackage{titletoc}
\titlecontents{section}[0pt]{\addvspace{2pt}\filright}
              {\contentspush{\thecontentslabel\ }}
              {}{\titlerule*[8pt]{.}\contentspage}

\newcommand{\sectionlabel}{~\makebox[3.5em][l]{\Roman{section}.}}
\newtheorem{myDef}{Definition}
\usepackage{amsmath}
\usepackage{extarrows}
\setlength{\parindent}{0pt}
\title{\huge{Computational Statistics\\
\large{Generalized Linear Mixed Model}}}
\author{Chenyin Gao\\[2pt]
\normalsize Dept. Statistics, Sun Yat-sen University \\[2pt]}
\date{}  % 这一行用来去掉默认的日期显示
\begin{document}
\maketitle
\tableofcontents
\section{Progect Summary}
\subsection{Model Notation}

In this project, we consider a clustering problem. Suppose we have observed n observations, each observation is a binary process, i.e. the response $Y_{ij} = 0$ or $1$, $i = 1,\cdots,n, j = 1,\cdots,T$. Here $n$ is the number of subjects and T is the length of observation. In general, T might vary across subjects, time points may also be different. In this project, however, we simply assume that all subjects have common time length and time points. We also assume that these subjects belong to two clusters. For each cluster, the conditional expectation of response variable is:
\begin{equation}
\begin{aligned}
P_{ij}=\mathbb{E}(Y_{ij}|U_i=1,X_{1,ij},Z_{1,i})=g^{-1}(\beta_1X_{1,ij}+Z_{1,i})\\
P_{ij}=\mathbb{E}(Y_{ij}|U_i=2,X_{2,ij},Z_{2,i})=g^{-1}(\beta_2X_{2,ij}+Z_{2,i})
\end{aligned}
\end{equation}

where $U$ is cluster membership, $X_{c,ij}$ and $Z_c,i(c = 1,2)$ are fixed and random effects, respectively. The link function $g^{-1}(x) =\frac{\exp(x)}{1+\exp(x)}$ is given. In a typical clustering problem, $U$ is usually unknown, and hence we treat $U$ as another random effect.

For random effects, we assume that $Z_{c,i}\sim N(0,\sigma_c^2)$ and $\mathbb{P}(U = 1)=\pi_1$ (then $\pi_2=1-\pi_1$). Then the parameter to be estimated is $\Omega=\{\beta_1,\beta_2,\sigma_1,\sigma_2,\pi_1\}$. Treating random effects as missing data, one can write the complete data likelihood function as

\begin{equation}
L(\Omega|Y_{ij},U_i,Z_{U_i,i})=\prod_{i=1}^n\prod_{c=1}^2
\{\pi_cf_c(Z_{c,i})[\prod_{j=1}^T f_c(Y_{ij}|Z_{c,i})]\}^{w_{ic}}
\end{equation}

where $f_c(Z_{c,i})$ is the density function of Normal distribution, $f_c(Y_{ij}|Z_{c,i})=\mathbb{P}^{Y_{ij}}(1-\mathbb{P}_{ij})^{1-Y_{ij}}$.
$w_{ic}$ is the dummy variable of $U_i$ , i.e.

\[w_{ic}=
\left\{
\begin{array}
 {r@{\quad,\quad}l}
 1 & if~subject~$i$ ~belongs ~to~ cluster~ c \\
 0 & otherwise
\end{array}
\right.
\]

\subsection{Simulation Setup and Requirement}
Generate 100 simulations. In each simulation, set $n$ = 100 and $T$ = 10. The true values of parameter are: $\beta_1=1,\beta_2=1,\pi_1=0.6,\sigma_1=2$ and $\sigma_2=10$


Use N(0,1) to generate the fixed effect X, and use them for all 100 simulations and use MCEM to evaluate the loglikelihood function. In the E-step, perform K = 500 Gibbs sampling incorporated with a Metropolis-Hastings step, and drop the first 100 as a
burn-in procedure.

\section{Generalized Linear Mixed Model(GLMM)}
Given the simulation parameters: $n=100$,$T=10$,$\beta_1=\beta_2=1$,$\pi_1= 0.6,$ $\sigma_1 = 2$,$\sigma_2 = 10$, we could obtain,

Observed variables
$$
\textbf{Y}=\left[\begin{matrix}
 Y_{11}& Y_{12}&\cdots Y_{1T}\\
 Y_{21}& Y_{22}&\cdots Y_{2T}\\
 \vdots&\vdots&\ddots&\vdots\\
 Y_{n1}& Y_{n2}&\cdots Y_{nT})\end{matrix}
\right]=\left[\begin{matrix}
\textbf{Y}_1\\
\textbf{Y}_2\\
\vdots\\
\textbf{Y}_n\end{matrix}
\right]
$$

Additional unobserved or unobservable variables
$$
\textbf{U}=\left[\begin{matrix}
\textbf{U}_1\\
\textbf{U}_2\\
\vdots\\
\textbf{U}_n\end{matrix}
\right],
\textbf{Z}=\left[\begin{matrix}
\textbf{Z}_{U_1,1}\\
\textbf{Z}_{U_2,2}\\
\vdots\\
\textbf{Z}_{U_n,n}\end{matrix}
\right]
$$

Explanatory variables(fixed effect)
$$
X=\left|\begin{matrix}
 X_{U_1,11}&X_{U_1,12}&\cdots&X_{U_1,1T}\\
 X_{U_2,21}&X_{U_2,22}&\cdots&X_{U_2,2T}\\
 \vdots & \vdots &\ddots & \vdots \\
 X_{U_n,n1}&X_{U_n,n2}&\cdots&X_{U_n,nT}
 \end{matrix}
\right|
$$

\subsection{Complete Log-Likelihood}
Given the necesssary parameters for each component of $\Omega$, we could write the augmented logged liklihood as
\begin{equation*}
\small
\begin{aligned}
l(\Omega|\textbf{Y},\textbf{U},\textbf{Z})&=\ln L(\Omega|\textbf{Y},\textbf{U},\textbf{Z})\\
&=\ln\prod_{i=1}^n\prod_{c=1}^2
\{\pi_cf_c(Z_{c,i})[\prod_{j=1}^T f_c(Y_{ij}|Z_{c,i})]\}^{w_{ic}}\\
&=\sum_{i=1}^n\sum_{c=1}^2w_{ic}[\ln\pi_c+\ln f_c(Z_{c,i})+\sum_{j=1}^T\ln f_c(Y_{ij}|Z_{c,i})]\\
&=\sum_{i=1}^n\sum_{c=1}^2w_{ic}\{\ln\pi_c-\frac{Z_{c,i}^2}{2\sigma_c^2}-\frac{1}{2}\ln(2\pi\sigma_c^2)+\sum_{j=1}^T[Y_{ij}\ln P_{ij}+(1-Y_{ij})\ln(1-P_{ij})]\}\\
&=n\sum_{c=1}^2w_{ic}[\ln\pi_c-\frac{1}{2}\ln(2\pi\sigma_c^2)]-\sum_{i=1}^n\sum_{c=1}^2w_c\frac{Z_{c,i}^2}{2\sigma_c^2}\\
&+\sum_{i=1}^n\sum_{c=1}^2w_{ic}\sum_{j=1}^T[Y_{ij}(\beta_cX_{c,ij}+Z_{c,i})-Y_{ij}\ln(1+\exp(\beta_cX_{c,ij}+Z_{c,i}))-(1-Y_{ij})\ln(1+\exp(\beta_cX_{c,ij}+Z_{c,i}))]\\
&=n\sum_{c=1}^2w_{ic}[\ln\pi_c-\frac{1}{2}\ln(2\pi\sigma_c^2)]\\
&+\sum_{i=1}^n\sum_{c=1}^2w_c\{-\frac{Z^2_{c,i}}{2\sigma_c^2}+\sum_{j=1}^T[Y_{ij}(\beta_cX_{c,ij}+Z_{c,i})-\ln(1+\exp(\beta_cX_{c,ij}+Z_{c,i}))]\}
\end{aligned}
\end{equation*}

which is equal to
\begin{equation*}
\begin{aligned}
l(\Omega|\textbf{Y},\textbf{U},\textbf{Z})&=\sum_{i=1}^n\ln f_{(U_i,Z_{U_i,i})}(U_i,Z_{U_i,i}|\pi_c,\sigma_1,\sigma_2)+\sum_{i=1}^n\sum_{j=1}^T\ln f_{Y_{ij}|(U_i,Z_{U_i,i})}(Y_{ij}|(U_i,Z_{U_i,i}),\beta_1,\beta_2)\\
&=\triangleq\ln f_{(\textbf{U},\textbf{Z})}(\textbf{U},\textbf{Z}|\pi_c,\sigma_1,\sigma_2)+\ln f_{\textbf{Y}|(\textbf{U},\textbf{Z})}(\textbf{Y}|\textbf{U},\textbf{Z},\beta_1,\beta_2)
\end{aligned}
\end{equation*}
\newpage
\section{Monte Carlo Expectation Maximization}
\subsection{EM Algorithm}

By taking expectation of $\textbf{U}$ and $\textbf{Z}$ given $\textbf{Y}$ under the current estimate of the parameters $\Omega^{(t)}$, we could write the expected augmented logged likelihood as
\begin{equation*}
\begin{aligned}
Q(\Omega,\Omega^{(t)})&=\mathbb{E}_{(\textbf{U},\textbf{Z})|(\textbf{Y},\Omega^{(t)})}\ln L(\Omega|\textbf{Y},\textbf{U},\textbf{Z})\\
&=\mathbb{E}_{(\textbf{U},\textbf{Z})|(\textbf{Y},\Omega^{(t)})}\\
&=\sum_{i=1}\sum_{c=1}^2\mathbb{E}_{\textbf{U}|(\textbf{Y},\Omega^{(t)})}(w_{ic})[\ln\pi_c-\frac{1}{2}\ln(2\pi\sigma_c^2)]\\
&+\sum_{i=1}^n\sum_{c=1}^2\mathbb{E}_{(\textbf{U},\textbf{Z})|(\textbf{Y},\Omega^{(t)})}w_c\{-\frac{Z^2_{c,i}}{2\sigma_c^2}+\sum_{j=1}^T[Y_{ij}(\beta_cX_{c,ij}+Z_{c,i})-\ln(1+\exp(\beta_cX_{c,ij}+Z_{c,i}))]\}
\end{aligned}
\end{equation*}

Notice that in the expected log-likelihood, $\Omega^{(t)}$ could be decomposed into seperate component
\begin{equation*}
\begin{aligned}
Q(\Omega,\Omega^{(t)})&=\mathbb{E}_{(\textbf{U},\textbf{Z})|(\textbf{Y},\Omega^{(t)})}\ln f_{(\textbf{U},\textbf{Z})}(\textbf{U},\textbf{Z}|\pi_c,\sigma_1,\sigma_2)+\mathbb{E}_{(\textbf{U},\textbf{Z})|(\textbf{Y},\Omega^{(t)})}\ln f_{\textbf{Y}|(\textbf{U},\textbf{Z})}(\textbf{Y}|\textbf{U},\textbf{Z},\beta_1,\beta_2)\\
&=\triangleq P(\Omega,\Omega^{(t)})+R(\Omega,\Omega^{(t)})
\end{aligned}
\end{equation*}

\subsection{Monte Carlo Integrating}
In order to compute the integral above, we use Monte Carlo Integrating to approximate it. Suppose that $\{(\textbf{U}_{(k)},\textbf{Z}_{(k)},k=1,2,\cdots,K)\}\overset{i.i.d}{\sim}f_{(\textbf{U},\textbf{Z}|\textbf{Y})}(\textbf{U},\textbf{Z}|\textbf{Y}),\Omega)$ and we sample $m$ times to approximate.

Based on Mean Value Method
\begin{equation}
Q(\Omega,\Omega^{(t)})\approx \frac{1}{m}\sum_{k=1}^m\sum_{\footnotesize i=1,\\[0.1pt]
c=U_{(k),i}}\bigg[\ln\pi_c-\frac{1}{2}\ln(2\pi\sigma_c^2)-\frac{Z^2_{c,i}}{2\sigma_c^2}+\sum_{j=1}^T[Y_{ij}(\beta_cX_{c,ij}+Z_{c,i})-\ln(1+\exp(\beta_cX_{c,ij}+Z_{c,i}))\bigg]
\end{equation}

\subsection{MLE}
The partial derivatives of the parameters are given by
\begin{equation*}
\begin{aligned}
&\frac{\partial Q(\Omega,\Omega^{(t)})}{\partial\pi_1}=\frac{1}{m}\sum_{k=1}^m\sum_{i=1}^n\mathbb{I}_{\{U_{(k),i},i=1\}}\frac{1}{\pi_1}-\frac{1}{m}\sum_{k=1}^m\sum_{i=1}^n\mathbb{I}_{\{U_{(k),i},i=2\}}\frac{1}{1-\pi_1}\\
&\frac{\partial Q(\Omega,\Omega^{(t)})}{\partial\sigma_c^2}=\frac{1}{m}\sum_{k=1}^m\sum_{i=1}^n\mathbb{I}_{\{U_{(k),i=c}\}}(-\frac{1}{2\sigma_c^2}+\frac{Z_{(k),c,i}^2}{2\sigma_c^4})\\
&\frac{\partial Q(\Omega,\Omega^{(t)})}{\partial\beta_c}=\frac{1}{m}\sum_{k=1}^m\sum_{i=1}^n\mathbb{I}_{\{U_{(k),i=c}\}}\sum_{j=1}^T\bigg[Y_ijX_{c,ij}-\frac{X_{c,ij}\exp(\beta_cX_{c,ij}+Z_{(k),c,i})}{1+\exp(\beta_cX_{c,ij}+Z_{(k),c,i})}\bigg]
\end{aligned}
\end{equation*}

By setting the above partial derivative to 0, we get the maximum likelihood estimators
\begin{equation}
\begin{aligned}
&\hat{\pi}_1=\frac{1}{mn}\sum_{k=1}^m\sum_{i=1}^n\mathbb{I}_{\{U_{(k)},i=1\}}\\
&\hat{\sigma}_c = \sqrt{\frac{\sum_{k=1}^m\sum_{i=1}^n\mathbb{I}_{\{U_{(k)},i=c\}}Z^2_{(k),c,i}}{\sum_{k=1}^m\sum_{i=1}^n\mathbb{I}_{\{U_{(k),i=c}\}}}}
\end{aligned}
\end{equation}

To compute the MLE of $\beta_c$, we use direct numerical maximization proposed by Newton-Raphson Method. The second order partial derivative of $\beta_c$ is denoted as
\begin{equation}
\frac{\partial^2 Q(\Omega,\Omega^{(t)})}{\partial\beta_c^2}=-\frac{1}{m}\sum_{k=1}^m\sum_{i=1}^n\mathbb{I}_{\{U_{(k),i=c}\}}\sum_{j=1}^T\frac{X_{c,ij}^2\exp(\beta_cX_{c,ij}+Z_{(k),c,i})}{(1+\exp(\beta_cX_{c,ij}+Z_{(k),c,i}))^2}
\end{equation}

\begin{algorithm}[h]
\caption{Newton-Raphson Method}
\begin{algorithmic}[1]
\State Initialize $\hat{\beta}^{(0)}_c$
\State t$\gets$ 0
\State $\hat{\beta}_c^{(t+1)}\gets\hat{\beta}_c^{(t)}-\frac{\frac{\partial Q(\Omega,\Omega^{(t)})}{\partial\beta_c}|_{\hat{\beta}_c^{(t)}}}{\frac{\partial^2Q(\Omega,\Omega^{(t)})}{\partial\beta_c^2}|_{\hat{\beta}_c^{(t)}}}$
\State Repeat step 2-3 until convergence
\end{algorithmic}
\end{algorithm}

\section{Markov Chain Sampler}

Since it difficult to sample directly form multivariate distribution $f_{(\textbf{U},\textbf{Z}|\textbf{Y})}(\textbf{U},\textbf{Z}|\textbf{Y}),\Omega)$.We can use Gibbs Sampling, a Markov chain Monte Carlo (MCMC) algorithm to obtain a sequence of observations which are approximated from the multivariate distribution.

First, we need to calculate the conditional distributions
\begin{equation}
\frac{f_{(U_i,Z_{(U_i,i)}|\textbf{Y}_i)}(U_i,Z_{U_i,i}|\textbf{Y}_i,\Omega)}{f_{Z_{(U_i,i)}|\textbf{Y}_i}(Z_{U_i,i}|\textbf{Y}_i,\Omega)}=f_{U_i|(Z_{U_i},i,\textbf{Y}_i)}(U_i|Z_{U_i,i},\textbf{Y}_i)
\end{equation}

and
\begin{equation}
\frac{f_{(U_i,Z_{(U_i,i)}|\textbf{Y}_i)}(U_i,Z_{U_i,i}|\textbf{Y}_i,\Omega)}{f_{U_i|\textbf{Y}_i}(U_i|\textbf{Y}_i,\Omega)}=f_{Z_{U_i,i}|(U_i,\textbf{Y}_i)}(Z_{U_i,i}|(U_i,\textbf{Y}_i))
\end{equation}

Then, suppose that $(U_{(k),i},Z_{(k),U_{(k),i},i})$ is the $i$th component of the $k$th sample, we want to draw the $i$th component of the $(k+1)$th sample. We draw
\begin{equation*}
\begin{aligned}
U_{(k+1),i}&\sim f_{U_i|Z_{U_i,i},\textbf{Y}_i}(u|Z_{U_i,i},\textbf{Y}_i,\Omega)\\
Z_{(k+1),U_{(k+1),i},i}&\sim f_{Z_{U_i,i}|U_i,\textbf{Y}_i}(z|U_i,\textbf{Y}_i,\Omega)
\end{aligned}
\end{equation*}

\subsection{Metropolis-Hastings Algorithm}

To sample $Z_{(k+1),U_{(k+1),i},i}$ from $f_{Z_{U_i,i}|U_i,\textbf{Y}_i}(z|U_i,\textbf{Y}_i,\Omega)$, let $h_{Z_{U_{(k),i},i}}(z)$ be the candidate distribution. Since the candidate distribution should be similar to $f_{Z_{U_i,i}|U_i,\textbf{Y}_i}(z|U_i,\textbf{Y}_i,\Omega)$, we can choose $h_{Z_{U_{(k),i},i}}(z)=f_{U_i}(z|\Omega)$ and the acceptance function is
\begin{equation*}
A_{k,\textbf{Y}_i}(z,z^*)=\min\bigg[1,\frac{f_{Z_{U_i,i}|U_i,\textbf{Y}_i}(z^*|U_i,\textbf{Y}_i,\Omega)f_{U_i}(z|\Omega)}{f_{Z_{U_i,i}|U_i,\textbf{Y}_i}(z|U_i,\textbf{Y}_i,\Omega)f_{U_i}(z^*|\Omega)}\bigg]
\end{equation*}

where $\frac{f_{Z_{U_i,i}|U_i,\textbf{Y}_i}(z^*|U_i,\textbf{Y}_i,\Omega)f_{U_i}(z|\Omega)}{f_{Z_{U_i,i}|U_i,\textbf{Y}_i}(z|U_i,\textbf{Y}_i,\Omega)f_{U_i}(z^*|\Omega)}$ can be written as,
\begin{equation*}
\begin{aligned}
\frac{f_{Z_{U_i,i}|U_i,\textbf{Y}_i}(z^*|U_i,\textbf{Y}_i,\Omega)f_{U_i}(z|\Omega)}{f_{Z_{U_i,i}|U_i,\textbf{Y}_i}(z|U_i,\textbf{Y}_i,\Omega)f_{U_i}(z^*|\Omega)}=\exp\bigg[\sum_{j=1}^TY_{ij}(z^*-z)\bigg]\prod_{j=1}^T\frac{1+\exp(\beta_iX_{ij}+z)}{1+\exp(\beta_iX_{ij}+z^*)}
\end{aligned}
\end{equation*}

We begin our Gibbs sampler incorporated a Metropolis-Hastings step as follow

\begin{algorithm}[h]
\caption{MCMC incorporated Metropolis-Hastings}
\small
\begin{algorithmic}
\For{i=1:n}
    \State Initialize$(U_{(0),i},Z_{(0),1,i},Z_{(0),2,i})$
    \For c=1:2
        \State $k\gets0$
        \For k=1:$K_2$
            \State Draw $z^*\sim f_c(z|\Omega)$
            \State Accept $z^*$ as $Z_{(k+1),c,i}$ with probability $A_{k,\textbf{Y}_i}(z,z^*)$; otherwise, retain the original $Z_{(k),c,i}$
        \EndFor
        Burn-in procedure and let the last K+1 samples be the final samples $\{Z_{(k),c,i},k=0,1,\cdots,K\}$
    \EndFor
    $k\gets0$
    \For k=1:$K$
        Draw $U_{(k+1),i}\sim f_{U_i|Z_{U_i},i,\textbf{Y}_i}(u|Z_{U_i},i,\textbf{Y}_i,\Omega)$
    \EndFor
    \State Let the last $m$ samples be the final samples $\{U_{(k),i},k=0,1,\cdots,K\}$
\EndFor
\State Burn-in procedure and return the $m$ samples $\{(U_{(i),i},Z_{(i),1,i},Z_{(i),2,i}),k=0,1,\cdots,m\}$
\end{algorithmic}
\label{al1}
\end{algorithm}

\subsection{MCEM}

Unfortunately, we don't known $f_{(\textbf{U},\textbf{Z})|\textbf{Y}}(\textbf{U},\textbf{Z}|\textbf{Y},\Omega)$, so we use$f_{(\textbf{U},\textbf{Z})|\textbf{Y}}(\textbf{U},\textbf{Z}|\textbf{Y},\Omega^{(t)})$ in the $(t+1)$th step to
estimate the distribution. To generate $\{(U_{(k)},Z_{(k)}), k=1,2,\cdots,m\} \overset{i.i.d}{\sim} f_{(\textbf{U},\textbf{Z})|\textbf{Y}}(\textbf{U},\textbf{Z}|\textbf{Y},\Omega^{(t)})$.

The Monte Carlo Expectation-Maximization Algorithm we use in every stimulation is given by

\begin{algorithm}[H]
\caption{MCEM}
\small
\begin{algorithmic}
    \State Start with the initial value $\Omega^{(0)}$. Set $t$=0.
    \State $\textsc{E-step}$:
    \State a. Generate $m$ samples $\{(U_{(i),i},Z_{(i),1,i},Z_{(i),2,i}),k=0,1,\cdots,m\}$ form  $f_{Z_{U_i,i}|U_i,\textbf{Y}_i}(z|U_i,\textbf{Y}_i,\Omega)$ through \autoref{al1}
    \State b.Calculate the partial derivatives of $Q(\Omega,\Omega^{(t)})$, the Monte Carlo estimator for every parameters.
    \State $\textsc{M-step}$
    \State $\Omega^{(t+1)}\gets\arg\max_{\Omega}Q(\Omega,\Omega^{(t)})$
    \State $t\gets t+1$
    \State Repeat step 2-6 until convergence and then output the maximum likelihood estimators $\Omega^{(t)}$.
    \end{algorithmic}
\label{al2}
\end{algorithm}

\section{Simulation}
For 1$Y_{ij} = 0$ or $1$, $i = 1,\cdots,n, j = 1,\cdots,T$, we begin our simulations as $n = 100$, $T = 10$ , $\beta_1=\beta_2=1$,$\sigma_1=5$,$\sigma_2=10$,$\pi=0.6$
\begin{enumerate}
\item we set the initial value as $\beta_1=\beta_2=0,\sigma_1=1,\sigma_2=5,\pi=0.8$and carry out 1000 experiments under different random seeds.
\item Perform variable step-size\footnote{We begin our Monte Carlo Estimation with small sample, as the EM iterate proceeding, we increase our Monte Carlo sampling times accordingly} Gibbs sampling in each EM iteration
\item Repeat 50 times EM iterate in each experiment
\item Demonstrate our first 100 experiments results and its MSE
\end{enumerate}


\end{document}
